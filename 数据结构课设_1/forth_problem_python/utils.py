# utils.py
import os

def get_all_text_files(folder):
    """è·å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
    files = []
    if not os.path.exists(folder):
        return files
        
    for filename in os.listdir(folder):
        if filename.endswith(('.txt', '.md', '.log')):
            files.append(os.path.join(folder, filename))
    return files

# crawler.py
import requests
from bs4 import BeautifulSoup
import time
import os
import random
from urllib.parse import quote

class WeiboCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def crawl_weibo(self, keyword, pages=3, save_folder="./data"):
        """çˆ¬å–å¾®åšæ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ä½¿ç”¨å¯èƒ½éœ€è¦å¤„ç†åçˆ¬è™«æœºåˆ¶ï¼‰"""
        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}")
        
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åçˆ¬è™«å¤„ç†
        # ç”±äºå¾®åšçš„åçˆ¬è™«æœºåˆ¶è¾ƒå¼ºï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„æ–°é—»ç½‘ç«™çˆ¬è™«ç¤ºä¾‹
        
        news_sites = [
            f"https://news.baidu.com/ns?word={quote(keyword)}&tn=news",
            f"https://www.toutiao.com/search/?keyword={quote(keyword)}",
        ]
        
        all_content = []
        
        # æ¨¡æ‹Ÿçˆ¬å–æ–°é—»å†…å®¹ï¼ˆå®é™…éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™è°ƒæ•´ï¼‰
        for page in range(pages):
            try:
                # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„çˆ¬è™«é€»è¾‘
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®
                sample_content = self._generate_sample_content(keyword, page)
                all_content.append(sample_content)
                
                print(f"âœ… çˆ¬å–ç¬¬ {page + 1} é¡µå®Œæˆ")
                time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿé¿å…è¢«å°
                
            except Exception as e:
                print(f"âŒ çˆ¬å–ç¬¬ {page + 1} é¡µå¤±è´¥: {e}")
                continue
        
        # ä¿å­˜çˆ¬å–çš„å†…å®¹
        if all_content:
            filename = f"crawled_{keyword}_{int(time.time())}.txt"
            filepath = os.path.join(save_folder, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# å…³é”®è¯: {keyword} çš„çˆ¬å–ç»“æœ\n\n")
                f.write("\n\n".join(all_content))
            
            print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        return len(all_content)
    
    def _generate_sample_content(self, keyword, page):
        """ç”Ÿæˆç¤ºä¾‹å†…å®¹ï¼ˆå®é™…é¡¹ç›®ä¸­æ›¿æ¢ä¸ºçœŸå®çˆ¬è™«é€»è¾‘ï¼‰"""
        samples = [
            f"è¿™æ˜¯å…³äº{keyword}çš„ç¬¬{page+1}é¡µå†…å®¹ã€‚{keyword}åœ¨å½“ä»Šç¤¾ä¼šä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥äº†è§£{keyword}çš„å„ä¸ªæ–¹é¢ã€‚",
            f"æ ¹æ®æœ€æ–°æŠ¥é“ï¼Œ{keyword}ç›¸å…³çš„ç ”ç©¶å–å¾—äº†é‡å¤§çªç ´ã€‚ä¸“å®¶è®¤ä¸º{keyword}å°†åœ¨æœªæ¥å‘æŒ¥æ›´å¤§ä½œç”¨ã€‚",
            f"åˆ†ææ˜¾ç¤ºï¼Œ{keyword}çš„å‘å±•è¶‹åŠ¿ä»¤äººå…³æ³¨ã€‚ä¸šå†…äººå£«å¯¹{keyword}çš„å‰æ™¯æŒä¹è§‚æ€åº¦ã€‚",
            f"å…³äº{keyword}çš„è®¨è®ºè¶Šæ¥è¶Šçƒ­çƒˆï¼Œç¤¾ä¼šå„ç•Œå¯¹{keyword}éƒ½ç»™äºˆäº†é«˜åº¦å…³æ³¨ã€‚",
            f"æœ€æ–°æ•°æ®è¡¨æ˜ï¼Œ{keyword}ç›¸å…³æŒ‡æ ‡å‘ˆç°ç§¯æå˜åŒ–ï¼Œ{keyword}çš„å½±å“åŠ›ä¸æ–­æ‰©å¤§ã€‚"
        ]
        
        return random.choice(samples) * 3  # å¢åŠ å†…å®¹é•¿åº¦
