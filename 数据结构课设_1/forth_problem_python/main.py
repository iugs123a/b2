# main.py
import os
import sys
from indexer import build_inverted_index
from searcher import SearchEngine
from display import display_results
from utils import get_all_text_files
from crawler import WeiboCrawler

def main():
    data_folder = "./data"
    
    # ç¡®ä¿æ•°æ®æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
    
    print("ğŸ” æœç´¢å¼•æ“å¯åŠ¨ä¸­...")
    print("1. ä½¿ç”¨æœ¬åœ°æ–‡æœ¬æ–‡ä»¶")
    print("2. çˆ¬å–å¾®åšæ•°æ®ï¼ˆæœªå®ç°ï¼‰")
    print("3. è°ƒè¯•æ¨¡å¼ï¼ˆæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰")
    choice = input("è¯·é€‰æ‹©æ¨¡å¼ (1/2/3): ")
    
    debug_mode = (choice == "3")
    
    if choice == "2":
        print("ğŸ•·ï¸ å¯åŠ¨å¾®åšçˆ¬è™«...")
        crawler = WeiboCrawler()
        keywords = input("è¯·è¾“å…¥è¦çˆ¬å–çš„å…³é”®è¯ï¼ˆå¤šä¸ªå…³é”®è¯ç”¨ç©ºæ ¼åˆ†éš”ï¼‰: ").split()
        pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°ï¼ˆå»ºè®®1-5é¡µï¼‰: "))
        
        for keyword in keywords:
            crawler.crawl_weibo(keyword, pages, data_folder)
    
    print("ğŸ” æ­£åœ¨åŠ è½½æ–‡ä»¶å’Œæ„å»ºå€’æ’ç´¢å¼•...")
    text_files = get_all_text_files(data_folder)
    
    if not text_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥dataæ–‡ä»¶å¤¹")
        return
    
    inverted_index = build_inverted_index(text_files, debug_mode)
    search_engine = SearchEngine(inverted_index)
    print(f"âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå…±ç´¢å¼•äº† {len(text_files)} ä¸ªæ–‡ä»¶")
    
    # æœç´¢å¾ªç¯
    while True:
        print("\n" + "="*50)
        keyword = input("è¯·è¾“å…¥å…³é”®è¯ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š")
        if keyword.lower() == "exit":
            break
        
        results = search_engine.search(keyword)
        if not results:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…å†…å®¹ã€‚")
            continue
        
        display_results(results, keyword, data_folder, debug_mode)

if __name__ == "__main__":
    main()
