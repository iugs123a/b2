# display.py
import jieba
import os
import chardet

def read_file_with_encoding(filepath):
    """æ™ºèƒ½è¯»å–æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç """
    try:
        # é¦–å…ˆå°è¯•å¸¸è§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16', 'ascii']
        
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    return f.read(), encoding
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœå¸¸è§ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨chardetæ£€æµ‹
        with open(filepath, 'rb') as f:
            raw_data = f.read()
            result = chardet.detect(raw_data)
            detected_encoding = result['encoding']
            
            if detected_encoding:
                try:
                    content = raw_data.decode(detected_encoding, errors='ignore')
                    return content, detected_encoding
                except Exception:
                    pass
        
        # æœ€åå°è¯•ç”¨utf-8å¼ºåˆ¶è¯»å–ï¼Œå¿½ç•¥é”™è¯¯
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read(), 'utf-8 (with errors ignored)'
            
    except Exception as e:
        return None, str(e)

def display_results(ranked_results, keyword, folder, debug_mode=False):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    print(f"\nğŸ” æœç´¢å…³é”®è¯: '{keyword}'")
    print(f"ğŸ“š æ‰¾åˆ° {len(ranked_results)} ä¸ªç›¸å…³æ–‡æ¡£")
    print("="*80)
    
    # å¯¹å…³é”®è¯è¿›è¡Œåˆ†è¯ï¼Œç”¨äºåŒ¹é…
    query_words = [word.strip() for word in jieba.cut(keyword) if word.strip() and len(word) > 1]
    
    if debug_mode:
        print(f"ğŸ”¤ å…³é”®è¯åˆ†è¯ç»“æœ: {query_words}")
    
    for i, (filename, positions, score) in enumerate(ranked_results[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ªç»“æœ
        print(f"\nğŸ“„ [{i}] æ–‡ä»¶: {filename}")
        print(f"ğŸ“Š ç›¸å…³æ€§åˆ†æ•°: {score:.4f}")
        print(f"ğŸ¯ å…³é”®è¯å‡ºç°ä½ç½®: {positions[:10]}{'...' if len(positions) > 10 else ''}")
        
        filepath = os.path.join(folder, filename)
        content, encoding_info = read_file_with_encoding(filepath)
        
        if content is None:
            print(f"  âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {encoding_info}")
            print("-" * 60)
            continue
        
        if debug_mode:
            print(f"  ğŸ“ æ–‡ä»¶ç¼–ç : {encoding_info}")
            print(f"  ğŸ“ æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # å¯¹å†…å®¹è¿›è¡Œåˆ†è¯
        words = list(jieba.cut(content))
        
        if debug_mode:
            print(f"  ğŸ”¤ åˆ†è¯åé•¿åº¦: {len(words)} è¯")
        
        # æ˜¾ç¤ºä¸Šä¸‹æ–‡ç‰‡æ®µ
        snippets_shown = 0
        shown_positions = set()  # è®°å½•å·²æ˜¾ç¤ºçš„ä½ç½®ï¼Œé¿å…é‡å¤
        
        valid_positions = [pos for pos in positions if pos < len(words)]
        
        if debug_mode:
            print(f"  ğŸ¯ æœ‰æ•ˆä½ç½®æ•°: {len(valid_positions)}/{len(positions)}")
        
        for pos in sorted(valid_positions):  # æŒ‰ä½ç½®æ’åº
            if snippets_shown >= 3:  # æœ€å¤šæ˜¾ç¤º3ä¸ªç‰‡æ®µ
                break
            
            # é¿å…æ˜¾ç¤ºé‡å¤çš„ç‰‡æ®µ
            if any(abs(pos - shown_pos) < 8 for shown_pos in shown_positions):
                if debug_mode:
                    print(f"    è·³è¿‡ä½ç½® {pos}ï¼ˆä¸å·²æ˜¾ç¤ºä½ç½®é‡å¤ï¼‰")
                continue
            
            shown_positions.add(pos)
            
            # æå–ä¸Šä¸‹æ–‡
            context_size = 12  # å‰åå„12ä¸ªè¯
            start = max(0, pos - context_size)
            end = min(len(words), pos + context_size + 1)
            
            # æ„å»ºä¸Šä¸‹æ–‡ç‰‡æ®µ
            context_words = words[start:end]
            target_word = words[pos]
            target_pos_in_context = pos - start
            
            if debug_mode:
                print(f"    ä½ç½® {pos}: ç›®æ ‡è¯='{target_word}', ä¸Šä¸‹æ–‡èŒƒå›´=[{start}:{end}]")
            
            # æ„å»ºå¸¦é«˜äº®çš„ç‰‡æ®µ
            highlighted_context = []
            for j, word in enumerate(context_words):
                if j == target_pos_in_context:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢è¯çš„ä¸€éƒ¨åˆ†
                    if any(query_word in word or word in query_word for query_word in query_words):
                        highlighted_context.append(f"ã€{word}ã€‘")
                    else:
                        highlighted_context.append(f"ã€Š{word}ã€‹")  # ç”¨ä¸åŒç¬¦å·æ ‡è®°å…¶ä»–åŒ¹é…
                else:
                    highlighted_context.append(word)
            
            snippet = ''.join(highlighted_context)
            
            # æ·»åŠ çœç•¥å·
            prefix = "..." if start > 0 else ""
            suffix = "..." if end < len(words) else ""
            
            print(f"  ğŸ’¡ {prefix}{snippet}{suffix}")
            snippets_shown += 1
        
        if snippets_shown == 0:
            print(f"  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç‰‡æ®µ")
            if debug_mode:
                print(f"    æ–‡æ¡£è¯æ•°: {len(words)}")
                print(f"    ä½ç½®åˆ—è¡¨: {positions[:5]}...")
        
        print("-" * 60)
